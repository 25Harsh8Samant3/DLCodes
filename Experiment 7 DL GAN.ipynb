{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/harshsamant/Downloads/diabetes.csv')\n",
    "# data = pd.read_csv('/Iris.csv')\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "            'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = ['Outcome']\n",
    "X = data[features]\n",
    "y = data[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\tX = generator.predict(x_input)\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(n):\n",
    "  X = data.sample(n)\n",
    "\n",
    "  #generate class labels\n",
    "  y = np.ones((n, 1))\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-05-07 22:39:09.985061: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-05-07 22:39:09.985080: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-05-07 22:39:09.985085: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-05-07 22:39:09.985099: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-07 22:39:09.985111: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">279</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m165\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │           \u001b[38;5;34m480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m279\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">924</span> (3.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m924\u001b[0m (3.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">924</span> (3.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m924\u001b[0m (3.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def define_generator(latent_dim, n_outputs=9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model\n",
    "\n",
    "gen = define_generator(10)\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │           \u001b[38;5;34m250\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m1,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,601</span> (6.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,601\u001b[0m (6.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,601</span> (6.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,601\u001b[0m (6.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def define_discriminator(n_inputs=9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "discriminator1 = define_discriminator()\n",
    "discriminator1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "\tdiscriminator.trainable = False\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(generator)\n",
    "\tmodel.add(discriminator)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d_hist, g_hist):\n",
    "\tplt.subplot(1, 1, 1)\n",
    "\tplt.plot(d_hist, label='d')\n",
    "\tplt.plot(g_hist, label='gen')\n",
    "\t# plt.legend()\n",
    "\t# plot discriminator accuracy\n",
    "\t# pyplot.subplot(2, 1, 2)\n",
    "\t# pyplot.plot(a1_hist, label='acc-real')\n",
    "\t# pyplot.plot(a2_hist, label='acc-fake')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=250, n_batch=128):\n",
    "    half_batch = int(n_batch / 2)\n",
    "\n",
    "    d_history = []\n",
    "    g_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real)\n",
    "        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "\n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "        print(f'>Epoch {epoch+1}, Real_Loss_Disc: {d_loss_real:.4f} Fake_Loss_Disc=: {d_loss_fake:.3f} Disc_Loss: {d_loss:.3f} Gen_Loss: {g_loss_fake:.3f}')\n",
    "        d_history.append(d_loss)\n",
    "        g_history.append(g_loss_fake)\n",
    "    plot_history(d_history, g_history)\n",
    "    g_model.save('Generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "\tx_real, y_real = generate_real_samples(n)\n",
    "\t_, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "\tx_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "\t_, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "\tprint(epoch, acc_real, acc_fake)\n",
    "\tplt.scatter(x_real[:, 0], color='red')\n",
    "\tplt.scatter(x_fake[:, 0], color='blue')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 22:39:10.223759: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "/opt/miniconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:74: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Epoch 1, Real_Loss_Disc: 6.2057 Fake_Loss_Disc=: 3.497 Disc_Loss: 4.851 Gen_Loss: 0.712\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x35644b4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x35740ed40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      ">Epoch 2, Real_Loss_Disc: 3.8462 Fake_Loss_Disc=: 3.093 Disc_Loss: 3.469 Gen_Loss: 0.712\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 3, Real_Loss_Disc: 3.8298 Fake_Loss_Disc=: 3.314 Disc_Loss: 3.572 Gen_Loss: 0.691\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 4, Real_Loss_Disc: 3.4289 Fake_Loss_Disc=: 3.101 Disc_Loss: 3.265 Gen_Loss: 0.678\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 5, Real_Loss_Disc: 3.4362 Fake_Loss_Disc=: 3.178 Disc_Loss: 3.307 Gen_Loss: 0.667\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 6, Real_Loss_Disc: 3.3820 Fake_Loss_Disc=: 3.181 Disc_Loss: 3.281 Gen_Loss: 0.652\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 7, Real_Loss_Disc: 3.3837 Fake_Loss_Disc=: 3.211 Disc_Loss: 3.297 Gen_Loss: 0.639\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 8, Real_Loss_Disc: 3.4178 Fake_Loss_Disc=: 3.262 Disc_Loss: 3.340 Gen_Loss: 0.624\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 9, Real_Loss_Disc: 3.3950 Fake_Loss_Disc=: 3.258 Disc_Loss: 3.327 Gen_Loss: 0.612\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 10, Real_Loss_Disc: 3.3506 Fake_Loss_Disc=: 3.234 Disc_Loss: 3.292 Gen_Loss: 0.599\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 11, Real_Loss_Disc: 3.3035 Fake_Loss_Disc=: 3.201 Disc_Loss: 3.252 Gen_Loss: 0.591\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 12, Real_Loss_Disc: 3.3349 Fake_Loss_Disc=: 3.242 Disc_Loss: 3.289 Gen_Loss: 0.580\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 13, Real_Loss_Disc: 3.3295 Fake_Loss_Disc=: 3.243 Disc_Loss: 3.286 Gen_Loss: 0.572\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 14, Real_Loss_Disc: 3.3452 Fake_Loss_Disc=: 3.269 Disc_Loss: 3.307 Gen_Loss: 0.563\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 15, Real_Loss_Disc: 3.3885 Fake_Loss_Disc=: 3.314 Disc_Loss: 3.351 Gen_Loss: 0.550\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 16, Real_Loss_Disc: 3.3526 Fake_Loss_Disc=: 3.283 Disc_Loss: 3.318 Gen_Loss: 0.542\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 17, Real_Loss_Disc: 3.3941 Fake_Loss_Disc=: 3.331 Disc_Loss: 3.363 Gen_Loss: 0.534\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 18, Real_Loss_Disc: 3.4594 Fake_Loss_Disc=: 3.401 Disc_Loss: 3.430 Gen_Loss: 0.525\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 19, Real_Loss_Disc: 3.4260 Fake_Loss_Disc=: 3.371 Disc_Loss: 3.399 Gen_Loss: 0.519\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 20, Real_Loss_Disc: 3.4339 Fake_Loss_Disc=: 3.380 Disc_Loss: 3.407 Gen_Loss: 0.512\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 21, Real_Loss_Disc: 3.4716 Fake_Loss_Disc=: 3.419 Disc_Loss: 3.445 Gen_Loss: 0.504\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 22, Real_Loss_Disc: 3.4737 Fake_Loss_Disc=: 3.425 Disc_Loss: 3.449 Gen_Loss: 0.496\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 23, Real_Loss_Disc: 3.4647 Fake_Loss_Disc=: 3.420 Disc_Loss: 3.442 Gen_Loss: 0.490\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 24, Real_Loss_Disc: 3.4634 Fake_Loss_Disc=: 3.420 Disc_Loss: 3.442 Gen_Loss: 0.484\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 25, Real_Loss_Disc: 3.4894 Fake_Loss_Disc=: 3.446 Disc_Loss: 3.468 Gen_Loss: 0.478\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 26, Real_Loss_Disc: 3.4375 Fake_Loss_Disc=: 3.398 Disc_Loss: 3.418 Gen_Loss: 0.471\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 27, Real_Loss_Disc: 3.4248 Fake_Loss_Disc=: 3.388 Disc_Loss: 3.406 Gen_Loss: 0.465\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 28, Real_Loss_Disc: 3.4743 Fake_Loss_Disc=: 3.439 Disc_Loss: 3.457 Gen_Loss: 0.459\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 29, Real_Loss_Disc: 3.5189 Fake_Loss_Disc=: 3.483 Disc_Loss: 3.501 Gen_Loss: 0.453\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 30, Real_Loss_Disc: 3.5584 Fake_Loss_Disc=: 3.524 Disc_Loss: 3.541 Gen_Loss: 0.447\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 31, Real_Loss_Disc: 3.5792 Fake_Loss_Disc=: 3.547 Disc_Loss: 3.563 Gen_Loss: 0.442\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 32, Real_Loss_Disc: 3.5825 Fake_Loss_Disc=: 3.550 Disc_Loss: 3.566 Gen_Loss: 0.436\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 33, Real_Loss_Disc: 3.6180 Fake_Loss_Disc=: 3.589 Disc_Loss: 3.603 Gen_Loss: 0.431\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 34, Real_Loss_Disc: 3.6095 Fake_Loss_Disc=: 3.581 Disc_Loss: 3.595 Gen_Loss: 0.426\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 35, Real_Loss_Disc: 3.6202 Fake_Loss_Disc=: 3.592 Disc_Loss: 3.606 Gen_Loss: 0.421\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 36, Real_Loss_Disc: 3.6311 Fake_Loss_Disc=: 3.603 Disc_Loss: 3.617 Gen_Loss: 0.416\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 37, Real_Loss_Disc: 3.6524 Fake_Loss_Disc=: 3.626 Disc_Loss: 3.639 Gen_Loss: 0.412\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 38, Real_Loss_Disc: 3.6633 Fake_Loss_Disc=: 3.640 Disc_Loss: 3.651 Gen_Loss: 0.407\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 39, Real_Loss_Disc: 3.6965 Fake_Loss_Disc=: 3.673 Disc_Loss: 3.685 Gen_Loss: 0.403\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 40, Real_Loss_Disc: 3.6966 Fake_Loss_Disc=: 3.672 Disc_Loss: 3.684 Gen_Loss: 0.399\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 41, Real_Loss_Disc: 3.6837 Fake_Loss_Disc=: 3.660 Disc_Loss: 3.672 Gen_Loss: 0.395\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 42, Real_Loss_Disc: 3.7249 Fake_Loss_Disc=: 3.701 Disc_Loss: 3.713 Gen_Loss: 0.390\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 43, Real_Loss_Disc: 3.7389 Fake_Loss_Disc=: 3.719 Disc_Loss: 3.729 Gen_Loss: 0.386\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 44, Real_Loss_Disc: 3.7549 Fake_Loss_Disc=: 3.734 Disc_Loss: 3.744 Gen_Loss: 0.382\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 45, Real_Loss_Disc: 3.7656 Fake_Loss_Disc=: 3.745 Disc_Loss: 3.756 Gen_Loss: 0.378\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 46, Real_Loss_Disc: 3.7823 Fake_Loss_Disc=: 3.762 Disc_Loss: 3.772 Gen_Loss: 0.374\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 47, Real_Loss_Disc: 3.7966 Fake_Loss_Disc=: 3.777 Disc_Loss: 3.787 Gen_Loss: 0.370\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 48, Real_Loss_Disc: 3.7977 Fake_Loss_Disc=: 3.778 Disc_Loss: 3.788 Gen_Loss: 0.366\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 49, Real_Loss_Disc: 3.8047 Fake_Loss_Disc=: 3.785 Disc_Loss: 3.795 Gen_Loss: 0.362\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 50, Real_Loss_Disc: 3.8010 Fake_Loss_Disc=: 3.782 Disc_Loss: 3.792 Gen_Loss: 0.358\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 51, Real_Loss_Disc: 3.8058 Fake_Loss_Disc=: 3.790 Disc_Loss: 3.798 Gen_Loss: 0.355\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 52, Real_Loss_Disc: 3.8212 Fake_Loss_Disc=: 3.805 Disc_Loss: 3.813 Gen_Loss: 0.351\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 53, Real_Loss_Disc: 3.8152 Fake_Loss_Disc=: 3.798 Disc_Loss: 3.807 Gen_Loss: 0.348\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 54, Real_Loss_Disc: 3.8065 Fake_Loss_Disc=: 3.791 Disc_Loss: 3.799 Gen_Loss: 0.344\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 55, Real_Loss_Disc: 3.8321 Fake_Loss_Disc=: 3.817 Disc_Loss: 3.824 Gen_Loss: 0.341\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 56, Real_Loss_Disc: 3.8296 Fake_Loss_Disc=: 3.814 Disc_Loss: 3.822 Gen_Loss: 0.338\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 57, Real_Loss_Disc: 3.8308 Fake_Loss_Disc=: 3.816 Disc_Loss: 3.824 Gen_Loss: 0.334\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 58, Real_Loss_Disc: 3.8433 Fake_Loss_Disc=: 3.830 Disc_Loss: 3.837 Gen_Loss: 0.331\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 59, Real_Loss_Disc: 3.8345 Fake_Loss_Disc=: 3.822 Disc_Loss: 3.828 Gen_Loss: 0.328\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 60, Real_Loss_Disc: 3.8405 Fake_Loss_Disc=: 3.827 Disc_Loss: 3.834 Gen_Loss: 0.325\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 61, Real_Loss_Disc: 3.8505 Fake_Loss_Disc=: 3.838 Disc_Loss: 3.844 Gen_Loss: 0.322\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 62, Real_Loss_Disc: 3.8707 Fake_Loss_Disc=: 3.858 Disc_Loss: 3.865 Gen_Loss: 0.319\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 63, Real_Loss_Disc: 3.8856 Fake_Loss_Disc=: 3.874 Disc_Loss: 3.880 Gen_Loss: 0.316\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 64, Real_Loss_Disc: 3.8943 Fake_Loss_Disc=: 3.882 Disc_Loss: 3.888 Gen_Loss: 0.313\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 65, Real_Loss_Disc: 3.9123 Fake_Loss_Disc=: 3.902 Disc_Loss: 3.907 Gen_Loss: 0.310\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 66, Real_Loss_Disc: 3.9241 Fake_Loss_Disc=: 3.913 Disc_Loss: 3.919 Gen_Loss: 0.307\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 67, Real_Loss_Disc: 3.9219 Fake_Loss_Disc=: 3.912 Disc_Loss: 3.917 Gen_Loss: 0.304\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 68, Real_Loss_Disc: 3.9274 Fake_Loss_Disc=: 3.916 Disc_Loss: 3.922 Gen_Loss: 0.302\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 69, Real_Loss_Disc: 3.9413 Fake_Loss_Disc=: 3.931 Disc_Loss: 3.936 Gen_Loss: 0.299\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 70, Real_Loss_Disc: 3.9446 Fake_Loss_Disc=: 3.934 Disc_Loss: 3.939 Gen_Loss: 0.296\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 71, Real_Loss_Disc: 3.9564 Fake_Loss_Disc=: 3.947 Disc_Loss: 3.952 Gen_Loss: 0.294\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 72, Real_Loss_Disc: 3.9672 Fake_Loss_Disc=: 3.959 Disc_Loss: 3.963 Gen_Loss: 0.291\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 73, Real_Loss_Disc: 3.9654 Fake_Loss_Disc=: 3.957 Disc_Loss: 3.961 Gen_Loss: 0.288\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 74, Real_Loss_Disc: 3.9704 Fake_Loss_Disc=: 3.961 Disc_Loss: 3.966 Gen_Loss: 0.286\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 75, Real_Loss_Disc: 3.9891 Fake_Loss_Disc=: 3.980 Disc_Loss: 3.985 Gen_Loss: 0.283\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 76, Real_Loss_Disc: 3.9921 Fake_Loss_Disc=: 3.985 Disc_Loss: 3.988 Gen_Loss: 0.281\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 77, Real_Loss_Disc: 4.0017 Fake_Loss_Disc=: 3.992 Disc_Loss: 3.997 Gen_Loss: 0.279\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 78, Real_Loss_Disc: 4.0138 Fake_Loss_Disc=: 4.005 Disc_Loss: 4.009 Gen_Loss: 0.276\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 79, Real_Loss_Disc: 4.0112 Fake_Loss_Disc=: 4.004 Disc_Loss: 4.008 Gen_Loss: 0.274\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 80, Real_Loss_Disc: 4.0135 Fake_Loss_Disc=: 4.006 Disc_Loss: 4.010 Gen_Loss: 0.272\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 81, Real_Loss_Disc: 4.0240 Fake_Loss_Disc=: 4.016 Disc_Loss: 4.020 Gen_Loss: 0.269\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 82, Real_Loss_Disc: 4.0332 Fake_Loss_Disc=: 4.026 Disc_Loss: 4.030 Gen_Loss: 0.267\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 83, Real_Loss_Disc: 4.0408 Fake_Loss_Disc=: 4.033 Disc_Loss: 4.037 Gen_Loss: 0.265\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 84, Real_Loss_Disc: 4.0477 Fake_Loss_Disc=: 4.041 Disc_Loss: 4.044 Gen_Loss: 0.263\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 85, Real_Loss_Disc: 4.0672 Fake_Loss_Disc=: 4.061 Disc_Loss: 4.064 Gen_Loss: 0.261\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 86, Real_Loss_Disc: 4.0744 Fake_Loss_Disc=: 4.067 Disc_Loss: 4.071 Gen_Loss: 0.259\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 87, Real_Loss_Disc: 4.0679 Fake_Loss_Disc=: 4.062 Disc_Loss: 4.065 Gen_Loss: 0.256\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 88, Real_Loss_Disc: 4.0784 Fake_Loss_Disc=: 4.073 Disc_Loss: 4.076 Gen_Loss: 0.254\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 89, Real_Loss_Disc: 4.0812 Fake_Loss_Disc=: 4.077 Disc_Loss: 4.079 Gen_Loss: 0.252\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 90, Real_Loss_Disc: 4.0990 Fake_Loss_Disc=: 4.094 Disc_Loss: 4.097 Gen_Loss: 0.250\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 91, Real_Loss_Disc: 4.1230 Fake_Loss_Disc=: 4.117 Disc_Loss: 4.120 Gen_Loss: 0.248\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 92, Real_Loss_Disc: 4.1311 Fake_Loss_Disc=: 4.126 Disc_Loss: 4.128 Gen_Loss: 0.246\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 93, Real_Loss_Disc: 4.1278 Fake_Loss_Disc=: 4.123 Disc_Loss: 4.125 Gen_Loss: 0.244\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 94, Real_Loss_Disc: 4.1309 Fake_Loss_Disc=: 4.126 Disc_Loss: 4.129 Gen_Loss: 0.242\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 95, Real_Loss_Disc: 4.1381 Fake_Loss_Disc=: 4.135 Disc_Loss: 4.136 Gen_Loss: 0.240\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 96, Real_Loss_Disc: 4.1565 Fake_Loss_Disc=: 4.152 Disc_Loss: 4.154 Gen_Loss: 0.238\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 97, Real_Loss_Disc: 4.1533 Fake_Loss_Disc=: 4.149 Disc_Loss: 4.151 Gen_Loss: 0.237\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 98, Real_Loss_Disc: 4.1577 Fake_Loss_Disc=: 4.153 Disc_Loss: 4.155 Gen_Loss: 0.235\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 99, Real_Loss_Disc: 4.1590 Fake_Loss_Disc=: 4.156 Disc_Loss: 4.157 Gen_Loss: 0.233\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 100, Real_Loss_Disc: 4.1664 Fake_Loss_Disc=: 4.163 Disc_Loss: 4.165 Gen_Loss: 0.231\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 101, Real_Loss_Disc: 4.1808 Fake_Loss_Disc=: 4.177 Disc_Loss: 4.179 Gen_Loss: 0.229\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 102, Real_Loss_Disc: 4.1837 Fake_Loss_Disc=: 4.180 Disc_Loss: 4.182 Gen_Loss: 0.228\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 103, Real_Loss_Disc: 4.1885 Fake_Loss_Disc=: 4.185 Disc_Loss: 4.187 Gen_Loss: 0.226\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 104, Real_Loss_Disc: 4.1928 Fake_Loss_Disc=: 4.190 Disc_Loss: 4.191 Gen_Loss: 0.224\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 105, Real_Loss_Disc: 4.2035 Fake_Loss_Disc=: 4.200 Disc_Loss: 4.202 Gen_Loss: 0.223\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 106, Real_Loss_Disc: 4.2091 Fake_Loss_Disc=: 4.206 Disc_Loss: 4.208 Gen_Loss: 0.221\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 107, Real_Loss_Disc: 4.2135 Fake_Loss_Disc=: 4.210 Disc_Loss: 4.212 Gen_Loss: 0.219\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 108, Real_Loss_Disc: 4.2099 Fake_Loss_Disc=: 4.207 Disc_Loss: 4.208 Gen_Loss: 0.218\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 109, Real_Loss_Disc: 4.2245 Fake_Loss_Disc=: 4.221 Disc_Loss: 4.223 Gen_Loss: 0.216\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 110, Real_Loss_Disc: 4.2292 Fake_Loss_Disc=: 4.228 Disc_Loss: 4.228 Gen_Loss: 0.215\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 111, Real_Loss_Disc: 4.2409 Fake_Loss_Disc=: 4.239 Disc_Loss: 4.240 Gen_Loss: 0.213\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 112, Real_Loss_Disc: 4.2522 Fake_Loss_Disc=: 4.250 Disc_Loss: 4.251 Gen_Loss: 0.211\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 113, Real_Loss_Disc: 4.2561 Fake_Loss_Disc=: 4.252 Disc_Loss: 4.254 Gen_Loss: 0.210\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 114, Real_Loss_Disc: 4.2632 Fake_Loss_Disc=: 4.262 Disc_Loss: 4.263 Gen_Loss: 0.208\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 115, Real_Loss_Disc: 4.2650 Fake_Loss_Disc=: 4.263 Disc_Loss: 4.264 Gen_Loss: 0.207\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 116, Real_Loss_Disc: 4.2759 Fake_Loss_Disc=: 4.274 Disc_Loss: 4.275 Gen_Loss: 0.206\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 117, Real_Loss_Disc: 4.2905 Fake_Loss_Disc=: 4.290 Disc_Loss: 4.290 Gen_Loss: 0.204\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 118, Real_Loss_Disc: 4.2934 Fake_Loss_Disc=: 4.293 Disc_Loss: 4.293 Gen_Loss: 0.203\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 119, Real_Loss_Disc: 4.3062 Fake_Loss_Disc=: 4.304 Disc_Loss: 4.305 Gen_Loss: 0.201\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 120, Real_Loss_Disc: 4.3113 Fake_Loss_Disc=: 4.310 Disc_Loss: 4.310 Gen_Loss: 0.200\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 121, Real_Loss_Disc: 4.3114 Fake_Loss_Disc=: 4.309 Disc_Loss: 4.310 Gen_Loss: 0.199\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 122, Real_Loss_Disc: 4.3169 Fake_Loss_Disc=: 4.316 Disc_Loss: 4.317 Gen_Loss: 0.197\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 123, Real_Loss_Disc: 4.3244 Fake_Loss_Disc=: 4.323 Disc_Loss: 4.324 Gen_Loss: 0.196\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 124, Real_Loss_Disc: 4.3356 Fake_Loss_Disc=: 4.335 Disc_Loss: 4.335 Gen_Loss: 0.195\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 125, Real_Loss_Disc: 4.3430 Fake_Loss_Disc=: 4.341 Disc_Loss: 4.342 Gen_Loss: 0.193\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 126, Real_Loss_Disc: 4.3526 Fake_Loss_Disc=: 4.352 Disc_Loss: 4.352 Gen_Loss: 0.192\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 127, Real_Loss_Disc: 4.3595 Fake_Loss_Disc=: 4.359 Disc_Loss: 4.359 Gen_Loss: 0.191\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 128, Real_Loss_Disc: 4.3618 Fake_Loss_Disc=: 4.361 Disc_Loss: 4.361 Gen_Loss: 0.190\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 129, Real_Loss_Disc: 4.3653 Fake_Loss_Disc=: 4.365 Disc_Loss: 4.365 Gen_Loss: 0.189\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 130, Real_Loss_Disc: 4.3763 Fake_Loss_Disc=: 4.376 Disc_Loss: 4.376 Gen_Loss: 0.187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 131, Real_Loss_Disc: 4.3761 Fake_Loss_Disc=: 4.375 Disc_Loss: 4.376 Gen_Loss: 0.186\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 132, Real_Loss_Disc: 4.3845 Fake_Loss_Disc=: 4.383 Disc_Loss: 4.384 Gen_Loss: 0.185\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 133, Real_Loss_Disc: 4.3863 Fake_Loss_Disc=: 4.384 Disc_Loss: 4.385 Gen_Loss: 0.184\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 134, Real_Loss_Disc: 4.3919 Fake_Loss_Disc=: 4.391 Disc_Loss: 4.391 Gen_Loss: 0.183\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 135, Real_Loss_Disc: 4.3894 Fake_Loss_Disc=: 4.389 Disc_Loss: 4.389 Gen_Loss: 0.182\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 136, Real_Loss_Disc: 4.3936 Fake_Loss_Disc=: 4.393 Disc_Loss: 4.393 Gen_Loss: 0.180\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 137, Real_Loss_Disc: 4.3953 Fake_Loss_Disc=: 4.394 Disc_Loss: 4.395 Gen_Loss: 0.179\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 138, Real_Loss_Disc: 4.3970 Fake_Loss_Disc=: 4.397 Disc_Loss: 4.397 Gen_Loss: 0.178\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 139, Real_Loss_Disc: 4.4033 Fake_Loss_Disc=: 4.403 Disc_Loss: 4.403 Gen_Loss: 0.177\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 140, Real_Loss_Disc: 4.4108 Fake_Loss_Disc=: 4.411 Disc_Loss: 4.411 Gen_Loss: 0.176\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 141, Real_Loss_Disc: 4.4180 Fake_Loss_Disc=: 4.418 Disc_Loss: 4.418 Gen_Loss: 0.175\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 142, Real_Loss_Disc: 4.4277 Fake_Loss_Disc=: 4.428 Disc_Loss: 4.428 Gen_Loss: 0.174\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 143, Real_Loss_Disc: 4.4344 Fake_Loss_Disc=: 4.434 Disc_Loss: 4.434 Gen_Loss: 0.173\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 144, Real_Loss_Disc: 4.4440 Fake_Loss_Disc=: 4.445 Disc_Loss: 4.445 Gen_Loss: 0.172\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 145, Real_Loss_Disc: 4.4475 Fake_Loss_Disc=: 4.448 Disc_Loss: 4.448 Gen_Loss: 0.171\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 146, Real_Loss_Disc: 4.4560 Fake_Loss_Disc=: 4.457 Disc_Loss: 4.456 Gen_Loss: 0.170\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 147, Real_Loss_Disc: 4.4617 Fake_Loss_Disc=: 4.462 Disc_Loss: 4.462 Gen_Loss: 0.169\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 148, Real_Loss_Disc: 4.4620 Fake_Loss_Disc=: 4.463 Disc_Loss: 4.462 Gen_Loss: 0.168\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 149, Real_Loss_Disc: 4.4662 Fake_Loss_Disc=: 4.466 Disc_Loss: 4.466 Gen_Loss: 0.167\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 150, Real_Loss_Disc: 4.4685 Fake_Loss_Disc=: 4.470 Disc_Loss: 4.469 Gen_Loss: 0.166\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 151, Real_Loss_Disc: 4.4716 Fake_Loss_Disc=: 4.472 Disc_Loss: 4.472 Gen_Loss: 0.165\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 152, Real_Loss_Disc: 4.4829 Fake_Loss_Disc=: 4.483 Disc_Loss: 4.483 Gen_Loss: 0.164\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 153, Real_Loss_Disc: 4.4878 Fake_Loss_Disc=: 4.489 Disc_Loss: 4.488 Gen_Loss: 0.163\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 154, Real_Loss_Disc: 4.4928 Fake_Loss_Disc=: 4.494 Disc_Loss: 4.493 Gen_Loss: 0.162\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 155, Real_Loss_Disc: 4.5033 Fake_Loss_Disc=: 4.504 Disc_Loss: 4.504 Gen_Loss: 0.161\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 156, Real_Loss_Disc: 4.5060 Fake_Loss_Disc=: 4.507 Disc_Loss: 4.506 Gen_Loss: 0.160\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 157, Real_Loss_Disc: 4.5185 Fake_Loss_Disc=: 4.519 Disc_Loss: 4.519 Gen_Loss: 0.159\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 158, Real_Loss_Disc: 4.5298 Fake_Loss_Disc=: 4.531 Disc_Loss: 4.530 Gen_Loss: 0.158\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 159, Real_Loss_Disc: 4.5459 Fake_Loss_Disc=: 4.547 Disc_Loss: 4.547 Gen_Loss: 0.158\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 160, Real_Loss_Disc: 4.5529 Fake_Loss_Disc=: 4.555 Disc_Loss: 4.554 Gen_Loss: 0.157\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 161, Real_Loss_Disc: 4.5597 Fake_Loss_Disc=: 4.561 Disc_Loss: 4.560 Gen_Loss: 0.156\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 162, Real_Loss_Disc: 4.5636 Fake_Loss_Disc=: 4.564 Disc_Loss: 4.564 Gen_Loss: 0.155\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 163, Real_Loss_Disc: 4.5696 Fake_Loss_Disc=: 4.571 Disc_Loss: 4.570 Gen_Loss: 0.154\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 164, Real_Loss_Disc: 4.5798 Fake_Loss_Disc=: 4.582 Disc_Loss: 4.581 Gen_Loss: 0.153\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 165, Real_Loss_Disc: 4.5890 Fake_Loss_Disc=: 4.590 Disc_Loss: 4.590 Gen_Loss: 0.153\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 166, Real_Loss_Disc: 4.5907 Fake_Loss_Disc=: 4.591 Disc_Loss: 4.591 Gen_Loss: 0.152\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 167, Real_Loss_Disc: 4.5983 Fake_Loss_Disc=: 4.599 Disc_Loss: 4.599 Gen_Loss: 0.151\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 168, Real_Loss_Disc: 4.6088 Fake_Loss_Disc=: 4.609 Disc_Loss: 4.609 Gen_Loss: 0.150\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      ">Epoch 169, Real_Loss_Disc: 4.6170 Fake_Loss_Disc=: 4.617 Disc_Loss: 4.617 Gen_Loss: 0.149\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">Epoch 170, Real_Loss_Disc: 4.6198 Fake_Loss_Disc=: 4.621 Disc_Loss: 4.621 Gen_Loss: 0.149\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      ">Epoch 171, Real_Loss_Disc: 4.6275 Fake_Loss_Disc=: 4.629 Disc_Loss: 4.628 Gen_Loss: 0.148\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 172, Real_Loss_Disc: 4.6327 Fake_Loss_Disc=: 4.635 Disc_Loss: 4.634 Gen_Loss: 0.147\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 173, Real_Loss_Disc: 4.6460 Fake_Loss_Disc=: 4.647 Disc_Loss: 4.646 Gen_Loss: 0.146\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 174, Real_Loss_Disc: 4.6503 Fake_Loss_Disc=: 4.651 Disc_Loss: 4.651 Gen_Loss: 0.145\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 175, Real_Loss_Disc: 4.6579 Fake_Loss_Disc=: 4.659 Disc_Loss: 4.658 Gen_Loss: 0.145\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 176, Real_Loss_Disc: 4.6661 Fake_Loss_Disc=: 4.668 Disc_Loss: 4.667 Gen_Loss: 0.144\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 177, Real_Loss_Disc: 4.6709 Fake_Loss_Disc=: 4.673 Disc_Loss: 4.672 Gen_Loss: 0.143\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      ">Epoch 178, Real_Loss_Disc: 4.6801 Fake_Loss_Disc=: 4.681 Disc_Loss: 4.681 Gen_Loss: 0.143\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 179, Real_Loss_Disc: 4.6848 Fake_Loss_Disc=: 4.686 Disc_Loss: 4.686 Gen_Loss: 0.142\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 180, Real_Loss_Disc: 4.6913 Fake_Loss_Disc=: 4.693 Disc_Loss: 4.692 Gen_Loss: 0.141\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 181, Real_Loss_Disc: 4.6940 Fake_Loss_Disc=: 4.696 Disc_Loss: 4.695 Gen_Loss: 0.140\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 182, Real_Loss_Disc: 4.6990 Fake_Loss_Disc=: 4.701 Disc_Loss: 4.700 Gen_Loss: 0.140\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 183, Real_Loss_Disc: 4.7023 Fake_Loss_Disc=: 4.704 Disc_Loss: 4.703 Gen_Loss: 0.139\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 184, Real_Loss_Disc: 4.7141 Fake_Loss_Disc=: 4.716 Disc_Loss: 4.715 Gen_Loss: 0.138\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 185, Real_Loss_Disc: 4.7267 Fake_Loss_Disc=: 4.728 Disc_Loss: 4.727 Gen_Loss: 0.138\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 186, Real_Loss_Disc: 4.7304 Fake_Loss_Disc=: 4.732 Disc_Loss: 4.731 Gen_Loss: 0.137\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 187, Real_Loss_Disc: 4.7411 Fake_Loss_Disc=: 4.743 Disc_Loss: 4.742 Gen_Loss: 0.136\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 188, Real_Loss_Disc: 4.7465 Fake_Loss_Disc=: 4.749 Disc_Loss: 4.748 Gen_Loss: 0.136\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 189, Real_Loss_Disc: 4.7542 Fake_Loss_Disc=: 4.756 Disc_Loss: 4.755 Gen_Loss: 0.135\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 190, Real_Loss_Disc: 4.7549 Fake_Loss_Disc=: 4.758 Disc_Loss: 4.756 Gen_Loss: 0.134\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 191, Real_Loss_Disc: 4.7648 Fake_Loss_Disc=: 4.766 Disc_Loss: 4.766 Gen_Loss: 0.134\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 192, Real_Loss_Disc: 4.7696 Fake_Loss_Disc=: 4.771 Disc_Loss: 4.770 Gen_Loss: 0.133\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 193, Real_Loss_Disc: 4.7748 Fake_Loss_Disc=: 4.776 Disc_Loss: 4.775 Gen_Loss: 0.132\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 194, Real_Loss_Disc: 4.7746 Fake_Loss_Disc=: 4.776 Disc_Loss: 4.776 Gen_Loss: 0.132\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 195, Real_Loss_Disc: 4.7777 Fake_Loss_Disc=: 4.779 Disc_Loss: 4.779 Gen_Loss: 0.131\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 196, Real_Loss_Disc: 4.7830 Fake_Loss_Disc=: 4.785 Disc_Loss: 4.784 Gen_Loss: 0.131\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 197, Real_Loss_Disc: 4.7868 Fake_Loss_Disc=: 4.789 Disc_Loss: 4.788 Gen_Loss: 0.130\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 198, Real_Loss_Disc: 4.7870 Fake_Loss_Disc=: 4.789 Disc_Loss: 4.788 Gen_Loss: 0.129\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 199, Real_Loss_Disc: 4.7933 Fake_Loss_Disc=: 4.795 Disc_Loss: 4.794 Gen_Loss: 0.129\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 200, Real_Loss_Disc: 4.8059 Fake_Loss_Disc=: 4.808 Disc_Loss: 4.807 Gen_Loss: 0.128\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 201, Real_Loss_Disc: 4.8082 Fake_Loss_Disc=: 4.810 Disc_Loss: 4.809 Gen_Loss: 0.128\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 202, Real_Loss_Disc: 4.8103 Fake_Loss_Disc=: 4.812 Disc_Loss: 4.811 Gen_Loss: 0.127\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 203, Real_Loss_Disc: 4.8188 Fake_Loss_Disc=: 4.821 Disc_Loss: 4.820 Gen_Loss: 0.126\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 204, Real_Loss_Disc: 4.8256 Fake_Loss_Disc=: 4.827 Disc_Loss: 4.827 Gen_Loss: 0.126\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 205, Real_Loss_Disc: 4.8329 Fake_Loss_Disc=: 4.835 Disc_Loss: 4.834 Gen_Loss: 0.125\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 206, Real_Loss_Disc: 4.8380 Fake_Loss_Disc=: 4.839 Disc_Loss: 4.839 Gen_Loss: 0.125\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 207, Real_Loss_Disc: 4.8404 Fake_Loss_Disc=: 4.842 Disc_Loss: 4.841 Gen_Loss: 0.124\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 208, Real_Loss_Disc: 4.8460 Fake_Loss_Disc=: 4.848 Disc_Loss: 4.847 Gen_Loss: 0.124\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 209, Real_Loss_Disc: 4.8489 Fake_Loss_Disc=: 4.851 Disc_Loss: 4.850 Gen_Loss: 0.123\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 210, Real_Loss_Disc: 4.8569 Fake_Loss_Disc=: 4.859 Disc_Loss: 4.858 Gen_Loss: 0.123\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 211, Real_Loss_Disc: 4.8625 Fake_Loss_Disc=: 4.865 Disc_Loss: 4.864 Gen_Loss: 0.122\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 212, Real_Loss_Disc: 4.8683 Fake_Loss_Disc=: 4.870 Disc_Loss: 4.869 Gen_Loss: 0.121\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 213, Real_Loss_Disc: 4.8701 Fake_Loss_Disc=: 4.873 Disc_Loss: 4.872 Gen_Loss: 0.121\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 214, Real_Loss_Disc: 4.8694 Fake_Loss_Disc=: 4.872 Disc_Loss: 4.871 Gen_Loss: 0.120\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 215, Real_Loss_Disc: 4.8782 Fake_Loss_Disc=: 4.881 Disc_Loss: 4.880 Gen_Loss: 0.120\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 216, Real_Loss_Disc: 4.8859 Fake_Loss_Disc=: 4.889 Disc_Loss: 4.887 Gen_Loss: 0.119\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 217, Real_Loss_Disc: 4.8921 Fake_Loss_Disc=: 4.895 Disc_Loss: 4.893 Gen_Loss: 0.119\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 218, Real_Loss_Disc: 4.9033 Fake_Loss_Disc=: 4.906 Disc_Loss: 4.905 Gen_Loss: 0.118\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 219, Real_Loss_Disc: 4.9061 Fake_Loss_Disc=: 4.909 Disc_Loss: 4.908 Gen_Loss: 0.118\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 220, Real_Loss_Disc: 4.9075 Fake_Loss_Disc=: 4.910 Disc_Loss: 4.909 Gen_Loss: 0.117\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 221, Real_Loss_Disc: 4.9108 Fake_Loss_Disc=: 4.913 Disc_Loss: 4.912 Gen_Loss: 0.117\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 222, Real_Loss_Disc: 4.9161 Fake_Loss_Disc=: 4.919 Disc_Loss: 4.917 Gen_Loss: 0.116\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 223, Real_Loss_Disc: 4.9258 Fake_Loss_Disc=: 4.929 Disc_Loss: 4.927 Gen_Loss: 0.116\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 224, Real_Loss_Disc: 4.9302 Fake_Loss_Disc=: 4.933 Disc_Loss: 4.931 Gen_Loss: 0.115\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 225, Real_Loss_Disc: 4.9314 Fake_Loss_Disc=: 4.933 Disc_Loss: 4.932 Gen_Loss: 0.115\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 226, Real_Loss_Disc: 4.9353 Fake_Loss_Disc=: 4.938 Disc_Loss: 4.937 Gen_Loss: 0.114\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 227, Real_Loss_Disc: 4.9456 Fake_Loss_Disc=: 4.948 Disc_Loss: 4.947 Gen_Loss: 0.114\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 228, Real_Loss_Disc: 4.9505 Fake_Loss_Disc=: 4.952 Disc_Loss: 4.951 Gen_Loss: 0.113\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 229, Real_Loss_Disc: 4.9576 Fake_Loss_Disc=: 4.960 Disc_Loss: 4.959 Gen_Loss: 0.113\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 230, Real_Loss_Disc: 4.9594 Fake_Loss_Disc=: 4.962 Disc_Loss: 4.961 Gen_Loss: 0.113\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 231, Real_Loss_Disc: 4.9667 Fake_Loss_Disc=: 4.968 Disc_Loss: 4.967 Gen_Loss: 0.112\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 232, Real_Loss_Disc: 4.9717 Fake_Loss_Disc=: 4.975 Disc_Loss: 4.973 Gen_Loss: 0.112\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 233, Real_Loss_Disc: 4.9763 Fake_Loss_Disc=: 4.979 Disc_Loss: 4.977 Gen_Loss: 0.111\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 234, Real_Loss_Disc: 4.9818 Fake_Loss_Disc=: 4.986 Disc_Loss: 4.984 Gen_Loss: 0.111\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 235, Real_Loss_Disc: 4.9889 Fake_Loss_Disc=: 4.991 Disc_Loss: 4.990 Gen_Loss: 0.110\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 236, Real_Loss_Disc: 4.9976 Fake_Loss_Disc=: 5.001 Disc_Loss: 4.999 Gen_Loss: 0.110\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 237, Real_Loss_Disc: 5.0016 Fake_Loss_Disc=: 5.003 Disc_Loss: 5.002 Gen_Loss: 0.109\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 238, Real_Loss_Disc: 5.0085 Fake_Loss_Disc=: 5.010 Disc_Loss: 5.009 Gen_Loss: 0.109\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 239, Real_Loss_Disc: 5.0100 Fake_Loss_Disc=: 5.013 Disc_Loss: 5.011 Gen_Loss: 0.109\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 240, Real_Loss_Disc: 5.0168 Fake_Loss_Disc=: 5.019 Disc_Loss: 5.018 Gen_Loss: 0.108\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      ">Epoch 241, Real_Loss_Disc: 5.0233 Fake_Loss_Disc=: 5.026 Disc_Loss: 5.025 Gen_Loss: 0.108\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 242, Real_Loss_Disc: 5.0277 Fake_Loss_Disc=: 5.030 Disc_Loss: 5.029 Gen_Loss: 0.107\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      ">Epoch 243, Real_Loss_Disc: 5.0360 Fake_Loss_Disc=: 5.039 Disc_Loss: 5.037 Gen_Loss: 0.107\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 244, Real_Loss_Disc: 5.0429 Fake_Loss_Disc=: 5.046 Disc_Loss: 5.045 Gen_Loss: 0.106\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">Epoch 245, Real_Loss_Disc: 5.0495 Fake_Loss_Disc=: 5.052 Disc_Loss: 5.051 Gen_Loss: 0.106\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 246, Real_Loss_Disc: 5.0562 Fake_Loss_Disc=: 5.059 Disc_Loss: 5.057 Gen_Loss: 0.106\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 247, Real_Loss_Disc: 5.0598 Fake_Loss_Disc=: 5.063 Disc_Loss: 5.061 Gen_Loss: 0.105\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 248, Real_Loss_Disc: 5.0693 Fake_Loss_Disc=: 5.072 Disc_Loss: 5.071 Gen_Loss: 0.105\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 249, Real_Loss_Disc: 5.0720 Fake_Loss_Disc=: 5.074 Disc_Loss: 5.073 Gen_Loss: 0.104\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      ">Epoch 250, Real_Loss_Disc: 5.0746 Fake_Loss_Disc=: 5.077 Disc_Loss: 5.076 Gen_Loss: 0.104\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "latent_dim = 10\n",
    "discriminator = define_discriminator()\n",
    "generator = define_generator(latent_dim)\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "train(generator, discriminator, gan_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.889523</td>\n",
       "      <td>3.624552</td>\n",
       "      <td>1.618774</td>\n",
       "      <td>1.300754</td>\n",
       "      <td>3.345721</td>\n",
       "      <td>-0.927804</td>\n",
       "      <td>4.039406</td>\n",
       "      <td>-3.418354</td>\n",
       "      <td>-2.222330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.522061</td>\n",
       "      <td>3.567859</td>\n",
       "      <td>3.446243</td>\n",
       "      <td>2.440364</td>\n",
       "      <td>6.205279</td>\n",
       "      <td>0.525804</td>\n",
       "      <td>7.053171</td>\n",
       "      <td>-4.417117</td>\n",
       "      <td>-3.016030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.363348</td>\n",
       "      <td>2.162410</td>\n",
       "      <td>1.845306</td>\n",
       "      <td>1.223743</td>\n",
       "      <td>2.364271</td>\n",
       "      <td>-0.332172</td>\n",
       "      <td>3.055547</td>\n",
       "      <td>-2.167832</td>\n",
       "      <td>-1.546743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.396837</td>\n",
       "      <td>4.815635</td>\n",
       "      <td>3.004989</td>\n",
       "      <td>2.636297</td>\n",
       "      <td>5.054331</td>\n",
       "      <td>-0.093423</td>\n",
       "      <td>6.434397</td>\n",
       "      <td>-4.355885</td>\n",
       "      <td>-3.320166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.659602</td>\n",
       "      <td>2.966170</td>\n",
       "      <td>3.178018</td>\n",
       "      <td>1.500959</td>\n",
       "      <td>4.368052</td>\n",
       "      <td>-0.060302</td>\n",
       "      <td>5.199034</td>\n",
       "      <td>-3.041380</td>\n",
       "      <td>-1.979140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.374625</td>\n",
       "      <td>3.145884</td>\n",
       "      <td>2.664761</td>\n",
       "      <td>2.805292</td>\n",
       "      <td>4.661870</td>\n",
       "      <td>0.248463</td>\n",
       "      <td>5.587871</td>\n",
       "      <td>-3.697398</td>\n",
       "      <td>-3.299416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.996501</td>\n",
       "      <td>9.509398</td>\n",
       "      <td>4.567500</td>\n",
       "      <td>3.247872</td>\n",
       "      <td>7.986123</td>\n",
       "      <td>-1.692735</td>\n",
       "      <td>9.833943</td>\n",
       "      <td>-7.165871</td>\n",
       "      <td>-5.365058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.514470</td>\n",
       "      <td>6.321843</td>\n",
       "      <td>3.641929</td>\n",
       "      <td>2.469301</td>\n",
       "      <td>5.305247</td>\n",
       "      <td>-0.892662</td>\n",
       "      <td>6.735856</td>\n",
       "      <td>-6.415323</td>\n",
       "      <td>-4.223166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-2.436575</td>\n",
       "      <td>6.964614</td>\n",
       "      <td>3.502327</td>\n",
       "      <td>2.656956</td>\n",
       "      <td>6.203565</td>\n",
       "      <td>-1.543326</td>\n",
       "      <td>7.751662</td>\n",
       "      <td>-6.135615</td>\n",
       "      <td>-4.036727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.663859</td>\n",
       "      <td>3.262992</td>\n",
       "      <td>2.018409</td>\n",
       "      <td>2.458658</td>\n",
       "      <td>3.253175</td>\n",
       "      <td>-0.836494</td>\n",
       "      <td>4.508234</td>\n",
       "      <td>-3.415699</td>\n",
       "      <td>-2.791279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.426786</td>\n",
       "      <td>4.950764</td>\n",
       "      <td>2.356418</td>\n",
       "      <td>3.123373</td>\n",
       "      <td>4.128669</td>\n",
       "      <td>-0.885184</td>\n",
       "      <td>6.042720</td>\n",
       "      <td>-4.422589</td>\n",
       "      <td>-3.555174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-4.118402</td>\n",
       "      <td>6.683391</td>\n",
       "      <td>4.232546</td>\n",
       "      <td>4.056037</td>\n",
       "      <td>6.903447</td>\n",
       "      <td>0.073670</td>\n",
       "      <td>9.607486</td>\n",
       "      <td>-6.404241</td>\n",
       "      <td>-4.920117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2.689524</td>\n",
       "      <td>7.634346</td>\n",
       "      <td>1.909452</td>\n",
       "      <td>2.911868</td>\n",
       "      <td>6.282607</td>\n",
       "      <td>-1.911714</td>\n",
       "      <td>7.237148</td>\n",
       "      <td>-5.428239</td>\n",
       "      <td>-3.293417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.897120</td>\n",
       "      <td>3.248695</td>\n",
       "      <td>1.700201</td>\n",
       "      <td>1.365267</td>\n",
       "      <td>2.347634</td>\n",
       "      <td>-0.242227</td>\n",
       "      <td>2.815538</td>\n",
       "      <td>-2.712891</td>\n",
       "      <td>-1.889305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-2.731699</td>\n",
       "      <td>5.239250</td>\n",
       "      <td>3.416037</td>\n",
       "      <td>2.638149</td>\n",
       "      <td>4.303716</td>\n",
       "      <td>-0.211178</td>\n",
       "      <td>6.062832</td>\n",
       "      <td>-4.553231</td>\n",
       "      <td>-3.889289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.584024</td>\n",
       "      <td>2.793067</td>\n",
       "      <td>3.209539</td>\n",
       "      <td>2.834986</td>\n",
       "      <td>4.559272</td>\n",
       "      <td>0.683668</td>\n",
       "      <td>5.742782</td>\n",
       "      <td>-3.959107</td>\n",
       "      <td>-3.446895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.907868</td>\n",
       "      <td>2.666151</td>\n",
       "      <td>2.709713</td>\n",
       "      <td>2.495124</td>\n",
       "      <td>5.717605</td>\n",
       "      <td>1.442424</td>\n",
       "      <td>5.697332</td>\n",
       "      <td>-4.032868</td>\n",
       "      <td>-2.952317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.830512</td>\n",
       "      <td>4.013670</td>\n",
       "      <td>3.653997</td>\n",
       "      <td>2.137097</td>\n",
       "      <td>4.084584</td>\n",
       "      <td>0.085868</td>\n",
       "      <td>5.861140</td>\n",
       "      <td>-4.445930</td>\n",
       "      <td>-3.820041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.094468</td>\n",
       "      <td>5.491633</td>\n",
       "      <td>2.837053</td>\n",
       "      <td>2.613877</td>\n",
       "      <td>6.187538</td>\n",
       "      <td>-0.191689</td>\n",
       "      <td>7.351755</td>\n",
       "      <td>-4.388264</td>\n",
       "      <td>-2.582774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2.604282</td>\n",
       "      <td>5.030347</td>\n",
       "      <td>2.222410</td>\n",
       "      <td>2.162138</td>\n",
       "      <td>4.092334</td>\n",
       "      <td>-1.289418</td>\n",
       "      <td>5.289793</td>\n",
       "      <td>-4.822921</td>\n",
       "      <td>-3.183150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies   Glucose  BloodPressure  SkinThickness   Insulin       BMI  \\\n",
       "0     -1.889523  3.624552       1.618774       1.300754  3.345721 -0.927804   \n",
       "1     -1.522061  3.567859       3.446243       2.440364  6.205279  0.525804   \n",
       "2     -1.363348  2.162410       1.845306       1.223743  2.364271 -0.332172   \n",
       "3     -1.396837  4.815635       3.004989       2.636297  5.054331 -0.093423   \n",
       "4     -1.659602  2.966170       3.178018       1.500959  4.368052 -0.060302   \n",
       "5     -1.374625  3.145884       2.664761       2.805292  4.661870  0.248463   \n",
       "6     -3.996501  9.509398       4.567500       3.247872  7.986123 -1.692735   \n",
       "7     -2.514470  6.321843       3.641929       2.469301  5.305247 -0.892662   \n",
       "8     -2.436575  6.964614       3.502327       2.656956  6.203565 -1.543326   \n",
       "9     -1.663859  3.262992       2.018409       2.458658  3.253175 -0.836494   \n",
       "10    -2.426786  4.950764       2.356418       3.123373  4.128669 -0.885184   \n",
       "11    -4.118402  6.683391       4.232546       4.056037  6.903447  0.073670   \n",
       "12    -2.689524  7.634346       1.909452       2.911868  6.282607 -1.911714   \n",
       "13    -1.897120  3.248695       1.700201       1.365267  2.347634 -0.242227   \n",
       "14    -2.731699  5.239250       3.416037       2.638149  4.303716 -0.211178   \n",
       "15    -1.584024  2.793067       3.209539       2.834986  4.559272  0.683668   \n",
       "16    -0.907868  2.666151       2.709713       2.495124  5.717605  1.442424   \n",
       "17    -1.830512  4.013670       3.653997       2.137097  4.084584  0.085868   \n",
       "18    -0.094468  5.491633       2.837053       2.613877  6.187538 -0.191689   \n",
       "19    -2.604282  5.030347       2.222410       2.162138  4.092334 -1.289418   \n",
       "\n",
       "    DiabetesPedigreeFunction       Age   Outcome  \n",
       "0                   4.039406 -3.418354 -2.222330  \n",
       "1                   7.053171 -4.417117 -3.016030  \n",
       "2                   3.055547 -2.167832 -1.546743  \n",
       "3                   6.434397 -4.355885 -3.320166  \n",
       "4                   5.199034 -3.041380 -1.979140  \n",
       "5                   5.587871 -3.697398 -3.299416  \n",
       "6                   9.833943 -7.165871 -5.365058  \n",
       "7                   6.735856 -6.415323 -4.223166  \n",
       "8                   7.751662 -6.135615 -4.036727  \n",
       "9                   4.508234 -3.415699 -2.791279  \n",
       "10                  6.042720 -4.422589 -3.555174  \n",
       "11                  9.607486 -6.404241 -4.920117  \n",
       "12                  7.237148 -5.428239 -3.293417  \n",
       "13                  2.815538 -2.712891 -1.889305  \n",
       "14                  6.062832 -4.553231 -3.889289  \n",
       "15                  5.742782 -3.959107 -3.446895  \n",
       "16                  5.697332 -4.032868 -2.952317  \n",
       "17                  5.861140 -4.445930 -3.820041  \n",
       "18                  7.351755 -4.388264 -2.582774  \n",
       "19                  5.289793 -4.822921 -3.183150  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = load_model('Generator.h5')\n",
    "latent_points = generate_latent_points(10, 750)\n",
    "X = model.predict(latent_points)\n",
    "data_fake = pd.DataFrame(data=X,  columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "                            'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'])\n",
    "data_fake.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
